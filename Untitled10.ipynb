{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb2b42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\92001\\anaconda3\\lib\\site-packages (0.47.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (1.4.1.post1)\n",
      "Requirement already satisfied: pandas in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (2.0.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (23.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (0.57.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\92001\\anaconda3\\lib\\site-packages (from shap) (4.12.2)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (0.40.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\92001\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f7d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\92001\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from statsmodels) (1.24.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from statsmodels) (1.11.1)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from statsmodels) (2.0.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from statsmodels) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\92001\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\92001\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3060362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile app.py\n",
    "# import streamlit as st\n",
    "# import h2o\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import shap\n",
    "# from h2o.automl import H2OAutoML\n",
    "# from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "# from scipy.stats import zscore\n",
    "\n",
    "\n",
    "# h2o.init()\n",
    "\n",
    "# st.title(\"H2O AutoML with EDA and Feature Engineering\")\n",
    "\n",
    "# uploaded_file = st.file_uploader(\"Upload CSV Dataset\", type=[\"csv\"])\n",
    "\n",
    "# if uploaded_file:\n",
    "#     df = pd.read_csv(uploaded_file)\n",
    "#     df = df.loc[:, ~df.columns.duplicated()]\n",
    "#     df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "#     df.dropna(inplace=True)\n",
    "    \n",
    "#     df = df.dropna(subset=[\"Volume\"])  \n",
    "#     df[\"Volume\"] = pd.to_numeric(df[\"Volume\"], errors=\"coerce\")\n",
    "#     df.dropna(subset=[\"Volume\"], inplace=True)\n",
    "#     df = df[df[\"Volume\"] > 0]  \n",
    "\n",
    "    \n",
    "#     st.write(\"### Filter Data Using L0 and L1\")\n",
    "#     col_L0 = st.selectbox(\"Select Feature for L0 Filter\", ['None'] + list(df.columns), key=\"L0_feature\")\n",
    "#     if col_L0 != 'None':\n",
    "#         unique_vals_L0 = ['None'] + sorted(df[col_L0].dropna().unique())\n",
    "#         val_L0 = st.selectbox(f\"Select Value from {col_L0}\", unique_vals_L0, key=\"L0_value\")\n",
    "#         if val_L0 != 'None':\n",
    "#             df = df[df[col_L0] == val_L0]\n",
    "\n",
    "#     col_L1 = st.selectbox(\"Select Feature for L1 Filter\", ['None'] + list(df.columns), key=\"L1_feature\")\n",
    "#     if col_L1 != 'None':\n",
    "#         unique_vals_L1 = ['None'] + sorted(df[col_L1].dropna().unique())\n",
    "#         val_L1 = st.selectbox(f\"Select Value from {col_L1}\", unique_vals_L1, key=\"L1_value\")\n",
    "#         if val_L1 != 'None':\n",
    "#             df = df[df[col_L1] == val_L1]\n",
    "\n",
    "\n",
    "#     st.write(\"Filtered Dataset\", df.head())\n",
    "#     st.write(\"Dataset shape before AutoML:\", df.shape)\n",
    "\n",
    "    \n",
    "#     drop_cols = [\"D2\", \"D3\", \"D4\", \"D5\", \"D6\", \"AV1\", \"AV2\", \"AV3\", \"AV4\", \"AV5\", \"AV6\", \"EV1\", \"EV2\", \"EV3\", \"EV4\", \"EV5\", \"EV6\"]\n",
    "#     df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "#     if \"Date\" in df.columns:\n",
    "#         df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "#         df.dropna(subset=[\"Date\"], inplace=True)\n",
    "\n",
    "#     if \"SalesValue\" in df.columns and \"Volume\" in df.columns:\n",
    "#         df[\"Price\"] = df[\"SalesValue\"] / df[\"Volume\"]\n",
    "#         df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#         df.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "    \n",
    "#     df[\"Month\"] = df[\"Date\"].dt.month\n",
    "#     df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
    "#     #df[\"Day_of_Week\"] = df[\"Date\"].dt.dayofweek\n",
    "#     df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
    "#     #df[\"Is_Weekend\"] = (df[\"Day_of_Week\"] >= 5).astype(int)\n",
    "\n",
    "#     df[\"Volume_Lag_1M\"] = df[\"Volume\"].shift(1)\n",
    "#     df[\"Volume_Lag_3M\"] = df[\"Volume\"].shift(3)\n",
    "#     df[\"Volume_MA_3M\"] = df[\"Volume\"].rolling(window=3).mean()\n",
    "#     df.fillna(0, inplace=True)\n",
    "\n",
    "#     #df[\"Price^2\"] = df[\"Price\"] ** 2\n",
    "#     #df[\"D1^2\"] = df[\"D1\"] ** 2 if \"D1\" in df.columns else 0\n",
    "\n",
    "#     if df[\"Volume\"].isna().sum() > 0 or df.shape[0] < 10:\n",
    "#         st.error(\"Insufficient valid Volume data after processing. Please check your filters or upload a different dataset.\")\n",
    "#         st.stop()\n",
    "\n",
    "#     st.write(\"### Correlation Heatmap\")\n",
    "#     numeric_df = df.select_dtypes(include=[np.number])\n",
    "#     fig, ax = plt.subplots(figsize=(15, 8))  \n",
    "#     #cmap = sns.diverging_palette(0, 120, s=100, l=50, as_cmap=True)\n",
    "#     #sns.heatmap(numeric_df.corr(), annot=True, fmt=\".2f\", cmap=cmap, ax=ax, annot_kws={\"size\": 10})\n",
    "#     sns.heatmap(numeric_df.corr(), annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, ax=ax, annot_kws={\"size\": 10})\n",
    "#     st.pyplot(fig)\n",
    "    \n",
    "#     st.subheader(\"Target and Feature Selection\")\n",
    "#     y_variable = st.selectbox(\"Select Y-variable (Target)\", options=[\"None\"] + list(df.columns), index=0)\n",
    "#     if y_variable == \"None\":\n",
    "#         st.stop()\n",
    "\n",
    "#     x_variables = st.multiselect(\"Select X-variables (Features)\", options=[col for col in df.columns if col != y_variable])\n",
    "\n",
    "#     if \"Date\" in df.columns:\n",
    "#         df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "#         df[\"Year\"] = df[\"Date\"].dt.year\n",
    "#         df[\"Month\"] = df[\"Date\"].dt.month\n",
    "\n",
    "#         st.subheader(\"Time Series Plots\")\n",
    "#         with st.expander(\"Optional Time Filters (Year / Month)\", expanded=False):\n",
    "#             years = sorted(df[\"Year\"].dropna().unique())\n",
    "#             months = sorted(df[\"Month\"].dropna().unique())\n",
    "#             selected_year = st.selectbox(\"Select Year\", options=[\"All\"] + list(years), index=0)\n",
    "#             selected_month = st.selectbox(\"Select Month\", options=[\"All\"] + list(months), index=0)\n",
    "\n",
    "#         ts_df = df.copy()\n",
    "#         if selected_year != \"All\":\n",
    "#             ts_df = ts_df[ts_df[\"Year\"] == selected_year]\n",
    "#         if selected_month != \"All\":\n",
    "#             ts_df = ts_df[ts_df[\"Month\"] == selected_month]\n",
    "\n",
    "#         grouped_ts = ts_df.groupby(\"Date\")[[y_variable] + x_variables].mean(numeric_only=True).reset_index()\n",
    "\n",
    "#         for feature in x_variables:\n",
    "#             if feature not in grouped_ts.columns:\n",
    "#                 st.warning(f\"Skipping feature '{feature}' — not in grouped data.\")\n",
    "#                 continue\n",
    "#             if not pd.api.types.is_numeric_dtype(grouped_ts[feature]):\n",
    "#                 continue\n",
    "\n",
    "#             fig, ax1 = plt.subplots(figsize=(10, 4))\n",
    "#             ax1.set_xlabel(\"Date\")\n",
    "#             ax1.set_ylabel(y_variable, color=\"tab:red\")\n",
    "#             ax1.plot(grouped_ts[\"Date\"], grouped_ts[y_variable], color=\"tab:red\")\n",
    "#             ax1.tick_params(axis='y', labelcolor=\"tab:red\")\n",
    "\n",
    "#             ax2 = ax1.twinx()\n",
    "#             ax2.set_ylabel(feature, color=\"tab:blue\")\n",
    "#             ax2.plot(grouped_ts[\"Date\"], grouped_ts[feature], color=\"tab:blue\")\n",
    "#             ax2.tick_params(axis='y', labelcolor=\"tab:blue\")\n",
    "\n",
    "#             fig.tight_layout()\n",
    "#             st.pyplot(fig)\n",
    "\n",
    "#     st.subheader(\"Outlier Removal\")\n",
    "#     outlier_method = st.selectbox(\"Choose method\", [\"None\", \"Z-Score\", \"IQR\"])\n",
    "#     initial_size = df.shape[0]\n",
    "\n",
    "#     if outlier_method != \"None\":\n",
    "#         target_series = df[y_variable].astype(float)\n",
    "#         if outlier_method == \"Z-Score\":\n",
    "#             z_scores = np.abs(zscore(target_series))\n",
    "#             df = df[z_scores < 3]\n",
    "#         elif outlier_method == \"IQR\":\n",
    "#             Q1, Q3 = target_series.quantile([0.25, 0.75])\n",
    "#             IQR = Q3 - Q1\n",
    "#             df = df[(target_series >= Q1 - 1.5 * IQR) & (target_series <= Q3 + 1.5 * IQR)]\n",
    "#         st.write(f\"Outliers removed: {initial_size - df.shape[0]}\")\n",
    "\n",
    "#     st.subheader(\"Feature Transformations\")\n",
    "#     feature_transforms = {}\n",
    "#     for feature in x_variables:\n",
    "#         if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "#             transform = st.selectbox(f\"Transform for {feature}\", [\"None\", \"Log\", \"Power\", \"Standardize\"], key=f\"trans_{feature}\")\n",
    "#             feature_transforms[feature] = transform\n",
    "\n",
    "#     for feature, method in feature_transforms.items():\n",
    "#         try:\n",
    "#             if method == \"Log\":\n",
    "#                 df[feature] = np.log1p(df[feature])\n",
    "#             elif method == \"Power\":\n",
    "#                 df[feature] = np.power(df[feature], 0.5)\n",
    "#             elif method == \"Standardize\":\n",
    "#                 df[feature] = (df[feature] - df[feature].mean()) / df[feature].std()\n",
    "#         except Exception as e:\n",
    "#             st.warning(f\"Could not transform {feature}: {e}\")\n",
    "    \n",
    "#     monotone_constraints = {}\n",
    "#     if 'Price' in x_variables:\n",
    "#         monotone_constraints['Price'] = -1\n",
    "    \n",
    "#     df[\"Date\"] = df[\"Date\"].astype(str)\n",
    "#     model_cols = list(dict.fromkeys([y_variable] + x_variables + [\"Date\"]))\n",
    "#     df_h2o = h2o.H2OFrame(df[model_cols])\n",
    "\n",
    "#     for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "#         if col != \"Date\" and col in df_h2o.columns:\n",
    "#             df_h2o[col] = df_h2o[col].asfactor()\n",
    "\n",
    "#     x_vars = [col for col in df_h2o.columns if col != y_variable]\n",
    "#     train, test = df_h2o.split_frame(ratios=[0.8], seed=42)\n",
    "#     target= y_variable\n",
    "\n",
    "#     st.write(f\"Training rows: {train.nrows}, Testing rows: {test.nrows}\")\n",
    "\n",
    "\n",
    "#     if st.button(\"Run AutoML\"):\n",
    "#         aml = H2OAutoML(max_runtime_secs=1200, include_algos=[\"GBM\"], monotone_constraints=monotone_constraints, seed=42)\n",
    "#         aml.train(x=x_vars, y=y_variable, training_frame=train)\n",
    "        \n",
    "#         #st.write(\"### Raw AutoML Leaderboard\")\n",
    "#         #leaderboard_df = aml.leaderboard.as_data_frame()\n",
    "#         #st.dataframe(leaderboard_df)\n",
    "\n",
    "#         top_models = []\n",
    "\n",
    "#         def update_top_models(model_id):\n",
    "#             model = h2o.get_model(model_id)\n",
    "#             preds = model.predict(test).as_data_frame().values.flatten()\n",
    "#             actuals = test[target].as_data_frame().values.flatten()\n",
    "\n",
    "#             mask = ~np.isnan(preds) & ~np.isnan(actuals)\n",
    "#             preds, actuals = preds[mask], actuals[mask]\n",
    "\n",
    "#             if len(preds) == 0:\n",
    "#                 return\n",
    "\n",
    "#             rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "#             mape = mean_absolute_percentage_error(actuals, preds)\n",
    "\n",
    "\n",
    "#             valid_mask = (actuals >= 0) & (preds >= 0)\n",
    "#             rmsle = np.sqrt(mean_squared_error(np.log1p(actuals[valid_mask]), np.log1p(preds[valid_mask]))) if valid_mask.sum() > 0 else np.nan\n",
    "\n",
    "#             model_info = {\n",
    "#                 \"model_id\": model_id,\n",
    "#                 \"rmse\": rmse,\n",
    "#                 \"mse\": mean_squared_error(actuals, preds),\n",
    "#                 \"mae\": np.mean(np.abs(actuals - preds)),\n",
    "#                 \"rmsle\": rmsle,\n",
    "#                 \"mean_residual_deviance\": np.mean((actuals - preds) ** 2),\n",
    "#                 \"mape\": mape\n",
    "#             }\n",
    "\n",
    "#             top_models.append(model_info)\n",
    "#             top_models.sort(key=lambda x: x[\"rmse\"])\n",
    "#             top_models[:] = top_models[:5]\n",
    "\n",
    "#         leaderboard_df = aml.leaderboard.as_data_frame()\n",
    "#         if \"model_id\" in leaderboard_df.columns:\n",
    "#             for model_id in leaderboard_df[\"model_id\"]:\n",
    "#                 update_top_models(model_id)\n",
    "#         else:\n",
    "#             st.warning(\"No models found in leaderboard.\")\n",
    "\n",
    "    \n",
    "#         top_models_df = pd.DataFrame(top_models)\n",
    "#         st.write(\"### AutoML Top 5 Models\")\n",
    "#         st.dataframe(top_models_df)\n",
    "\n",
    "\n",
    "#         if len(top_models) > 0:\n",
    "#             overall_rmse = np.mean([m[\"rmse\"] for m in top_models])\n",
    "#             overall_mape = np.mean([m[\"mape\"] for m in top_models])\n",
    "#             st.write(f\"**Overall RMSE:** {overall_rmse}\")\n",
    "#             st.write(f\"**Overall MAPE:** {overall_mape}\")\n",
    "            \n",
    "\n",
    "\n",
    "#         shap_values = {}\n",
    "#         shap_values = {}\n",
    "#         model_preds = {}\n",
    "\n",
    "#         X_test_df = test.drop(y_variable).as_data_frame()\n",
    "#         y_test = test[y_variable].as_data_frame().values.flatten()\n",
    "\n",
    "#         for model_info in top_models:\n",
    "#             model_id = model_info[\"model_id\"]\n",
    "#             model = h2o.get_model(model_id)\n",
    "\n",
    "#             try:\n",
    "#                 if model.algo not in [\"gbm\", \"xgboost\"]:\n",
    "#                     raise ValueError(f\"Unsupported algorithm: {model.algo}\")\n",
    "\n",
    "#                 contrib = model.predict_contributions(test).as_data_frame()\n",
    "#                 bias = contrib[\"BiasTerm\"]\n",
    "#                 contrib = contrib.drop(\"BiasTerm\", axis=1)\n",
    "#                 contrib = contrib.reindex(columns=X_test_df.columns)\n",
    "#                 shap_values[model_id] = contrib.mean().to_frame(name=model_id).T  \n",
    "\n",
    "#                 model_preds[model_id] = model.predict(test).as_data_frame().values.flatten()\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 st.warning(f\"Failed SHAP for {model_id}: {e}\")\n",
    "\n",
    "#         if shap_values:\n",
    "#             shap_grid_df = pd.concat(shap_values.values(), keys=shap_values.keys()).T\n",
    "#             shap_grid_df.columns.name = None\n",
    "#             st.subheader(\"SHAP Values Grid (mean contributions)\")\n",
    "#             st.dataframe(shap_grid_df.style.format(\"{:.4f}\"), use_container_width=True)\n",
    "\n",
    "#         elasticity_results = {}\n",
    "#         for model_id, shap_df in shap_values.items():\n",
    "#             try:\n",
    "#                 shap_df = shap_df.T.squeeze() if isinstance(shap_df, pd.DataFrame) else shap_df\n",
    "#                 mean_prediction = np.mean(model_preds[model_id])\n",
    "#                 elasticities = shap_df / mean_prediction\n",
    "#                 elasticity_results[model_id] = elasticities\n",
    "#             except Exception as e:\n",
    "#                 st.warning(f\"Elasticity calculation failed for {model_id}: {e}\")\n",
    "\n",
    "#         if elasticity_results:\n",
    "#             elasticity_df = pd.DataFrame(elasticity_results)\n",
    "#             st.subheader(\"Elasticity Grid Across Models\")\n",
    "#             st.dataframe(elasticity_df.style.format(\"{:.5f}\"), use_container_width=True)\n",
    "\n",
    "#             fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#             sns.heatmap(elasticity_df, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n",
    "#             plt.title(\"Elasticities by Feature and Model\")\n",
    "#             st.pyplot(fig)\n",
    "\n",
    "#         st.subheader(\"Actual vs Predicted\")\n",
    "#         for model_id, preds in model_preds.items():\n",
    "#             fig, ax = plt.subplots()\n",
    "#             ax.plot(y_test, label=\"Actual\", color=\"blue\")\n",
    "#             ax.plot(preds, label=\"Predicted\", color=\"red\")\n",
    "#             ax.set_title(f\"Actual vs Predicted for {model_id}\")\n",
    "#             ax.legend()\n",
    "#             st.pyplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb0c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile app.py\n",
    "# import streamlit as st\n",
    "# import h2o\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import shap\n",
    "# from h2o.automl import H2OAutoML\n",
    "# from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "# from scipy.stats import zscore\n",
    "# from statsmodels.tsa.api import detrend\n",
    "\n",
    "# h2o.init(max_mem_size=\"8G\")\n",
    "\n",
    "# st.title(\"H2O AutoML with EDA and Feature Engineering\")\n",
    "\n",
    "# uploaded_file = st.file_uploader(\"Upload CSV Dataset\", type=[\"csv\"])\n",
    "\n",
    "# if uploaded_file:\n",
    "#     df = pd.read_csv(uploaded_file)\n",
    "#     df = df.loc[:, ~df.columns.duplicated()]\n",
    "#     df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "#     df.dropna(inplace=True)\n",
    "    \n",
    "#     df = df.dropna(subset=[\"Volume\"])  \n",
    "#     df[\"Volume\"] = pd.to_numeric(df[\"Volume\"], errors=\"coerce\")\n",
    "#     df.dropna(subset=[\"Volume\"], inplace=True)\n",
    "#     df = df[df[\"Volume\"] > 0]  \n",
    "\n",
    "#     st.write(\"### Filter Data Using L0 and L1\")\n",
    "#     col_L0 = st.selectbox(\"Select Feature for L0 Filter\", ['None'] + list(df.columns), key=\"L0_feature\")\n",
    "#     if col_L0 != 'None':\n",
    "#         unique_vals_L0 = ['None'] + sorted(df[col_L0].dropna().unique())\n",
    "#         val_L0 = st.selectbox(f\"Select Value from {col_L0}\", unique_vals_L0, key=\"L0_value\")\n",
    "#         if val_L0 != 'None':\n",
    "#             df = df[df[col_L0] == val_L0]\n",
    "\n",
    "#     col_L1 = st.selectbox(\"Select Feature for L1 Filter\", ['None'] + list(df.columns), key=\"L1_feature\")\n",
    "#     if col_L1 != 'None':\n",
    "#         unique_vals_L1 = ['None'] + sorted(df[col_L1].dropna().unique())\n",
    "#         val_L1 = st.selectbox(f\"Select Value from {col_L1}\", unique_vals_L1, key=\"L1_value\")\n",
    "#         if val_L1 != 'None':\n",
    "#             df = df[df[col_L1] == val_L1]\n",
    "\n",
    "#     st.write(\"Filtered Dataset\", df.head())\n",
    "#     st.write(\"Dataset shape before AutoML:\", df.shape)\n",
    "    \n",
    "#     drop_cols = [\"D2\", \"D3\", \"D4\", \"D5\", \"D6\", \"AV1\", \"AV2\", \"AV3\", \"AV4\", \"AV5\", \"AV6\", \"EV1\", \"EV2\", \"EV3\", \"EV4\", \"EV5\", \"EV6\"]\n",
    "#     df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "#     if \"Date\" in df.columns:\n",
    "#         df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "#         df.dropna(subset=[\"Date\"], inplace=True)\n",
    "\n",
    "#     # Modified Price Calculation\n",
    "#     if \"SalesValue\" in df.columns and \"Volume\" in df.columns:\n",
    "#         if \"ListPrice\" in df.columns:  # Prefer existing price if available\n",
    "#             df[\"Price\"] = df[\"ListPrice\"]\n",
    "#         else:\n",
    "#             # Detrend the price to remove time effects\n",
    "#             raw_price = df[\"SalesValue\"] / df[\"Volume\"]\n",
    "#             df[\"Price\"] = detrend(raw_price.values)\n",
    "#             df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#             df.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "#     # Time features\n",
    "#     df[\"Month\"] = df[\"Date\"].dt.month\n",
    "#     df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
    "#     df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
    "\n",
    "#     # Remove lagged volume features that might cause circularity\n",
    "#     # df[\"Volume_Lag_1M\"] = df[\"Volume\"].shift(1)  # Removed\n",
    "#     # df[\"Volume_Lag_3M\"] = df[\"Volume\"].shift(3)  # Removed\n",
    "#     # df[\"Volume_MA_3M\"] = df[\"Volume\"].rolling(window=3).mean()  # Removed\n",
    "#     df.fillna(0, inplace=True)\n",
    "\n",
    "#     st.write(\"### Correlation Heatmap\")\n",
    "#     numeric_df = df.select_dtypes(include=[np.number])\n",
    "#     fig, ax = plt.subplots(figsize=(15, 8))  \n",
    "#     sns.heatmap(numeric_df.corr(), annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, ax=ax, annot_kws={\"size\": 10})\n",
    "#     st.pyplot(fig)\n",
    "    \n",
    "#     st.subheader(\"Target and Feature Selection\")\n",
    "#     y_variable = st.selectbox(\"Select Y-variable (Target)\", options=[\"None\"] + list(df.columns), index=0)\n",
    "#     if y_variable == \"None\":\n",
    "#         st.stop()\n",
    "\n",
    "#     x_variables = st.multiselect(\"Select X-variables (Features)\", options=[col for col in df.columns if col != y_variable])\n",
    "\n",
    "#     if \"Date\" in df.columns:\n",
    "#         df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "#         df[\"Year\"] = df[\"Date\"].dt.year\n",
    "#         df[\"Month\"] = df[\"Date\"].dt.month\n",
    "\n",
    "#         st.subheader(\"Time Series Plots\")\n",
    "#         with st.expander(\"Optional Time Filters (Year / Month)\", expanded=False):\n",
    "#             years = sorted(df[\"Year\"].dropna().unique())\n",
    "#             months = sorted(df[\"Month\"].dropna().unique())\n",
    "#             selected_year = st.selectbox(\"Select Year\", options=[\"All\"] + list(years), index=0)\n",
    "#             selected_month = st.selectbox(\"Select Month\", options=[\"All\"] + list(months), index=0)\n",
    "\n",
    "#         ts_df = df.copy()\n",
    "#         if selected_year != \"All\":\n",
    "#             ts_df = ts_df[ts_df[\"Year\"] == selected_year]\n",
    "#         if selected_month != \"All\":\n",
    "#             ts_df = ts_df[ts_df[\"Month\"] == selected_month]\n",
    "\n",
    "#         grouped_ts = ts_df.groupby(\"Date\")[[y_variable] + x_variables].mean(numeric_only=True).reset_index()\n",
    "\n",
    "#         for feature in x_variables:\n",
    "#             if feature not in grouped_ts.columns:\n",
    "#                 st.warning(f\"Skipping feature '{feature}' — not in grouped data.\")\n",
    "#                 continue\n",
    "#             if not pd.api.types.is_numeric_dtype(grouped_ts[feature]):\n",
    "#                 continue\n",
    "\n",
    "#             fig, ax1 = plt.subplots(figsize=(10, 4))\n",
    "#             ax1.set_xlabel(\"Date\")\n",
    "#             ax1.set_ylabel(y_variable, color=\"tab:red\")\n",
    "#             ax1.plot(grouped_ts[\"Date\"], grouped_ts[y_variable], color=\"tab:red\")\n",
    "#             ax1.tick_params(axis='y', labelcolor=\"tab:red\")\n",
    "\n",
    "#             ax2 = ax1.twinx()\n",
    "#             ax2.set_ylabel(feature, color=\"tab:blue\")\n",
    "#             ax2.plot(grouped_ts[\"Date\"], grouped_ts[feature], color=\"tab:blue\")\n",
    "#             ax2.tick_params(axis='y', labelcolor=\"tab:blue\")\n",
    "\n",
    "#             fig.tight_layout()\n",
    "#             st.pyplot(fig)\n",
    "\n",
    "#     st.subheader(\"Outlier Removal\")\n",
    "#     outlier_method = st.selectbox(\"Choose method\", [\"None\", \"Z-Score\", \"IQR\"])\n",
    "#     initial_size = df.shape[0]\n",
    "\n",
    "#     if outlier_method != \"None\":\n",
    "#         target_series = df[y_variable].astype(float)\n",
    "#         if outlier_method == \"Z-Score\":\n",
    "#             z_scores = np.abs(zscore(target_series))\n",
    "#             df = df[z_scores < 3]\n",
    "#         elif outlier_method == \"IQR\":\n",
    "#             Q1, Q3 = target_series.quantile([0.25, 0.75])\n",
    "#             IQR = Q3 - Q1\n",
    "#             df = df[(target_series >= Q1 - 1.5 * IQR) & (target_series <= Q3 + 1.5 * IQR)]\n",
    "#         st.write(f\"Outliers removed: {initial_size - df.shape[0]}\")\n",
    "\n",
    "#     st.subheader(\"Feature Transformations\")\n",
    "#     feature_transforms = {}\n",
    "#     for feature in x_variables:\n",
    "#         if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "#             transform = st.selectbox(f\"Transform for {feature}\", [\"None\", \"Log\", \"Power\", \"Standardize\"], key=f\"trans_{feature}\")\n",
    "#             feature_transforms[feature] = transform\n",
    "\n",
    "#     for feature, method in feature_transforms.items():\n",
    "#         try:\n",
    "#             if method == \"Log\":\n",
    "#                 df[feature] = np.log1p(df[feature])\n",
    "#             elif method == \"Power\":\n",
    "#                 df[feature] = np.power(df[feature], 0.5)\n",
    "#             elif method == \"Standardize\":\n",
    "#                 df[feature] = (df[feature] - df[feature].mean()) / df[feature].std()\n",
    "#         except Exception as e:\n",
    "#             st.warning(f\"Could not transform {feature}: {e}\")\n",
    "    \n",
    "    \n",
    "#     monotone_constraints = None\n",
    "#     if 'Price' in x_variables:\n",
    "#         monotone_constraints = {'Price': -1}  \n",
    "    \n",
    "#     df[\"Date\"] = df[\"Date\"].astype(str)\n",
    "#     model_cols = list(dict.fromkeys([y_variable] + x_variables + [\"Date\"]))\n",
    "#     df_h2o = h2o.H2OFrame(df[model_cols])\n",
    "\n",
    "#     for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "#         if col != \"Date\" and col in df_h2o.columns:\n",
    "#             df_h2o[col] = df_h2o[col].asfactor()\n",
    "\n",
    "#     x_vars = [col for col in df_h2o.columns if col != y_variable]\n",
    "#     train, test = df_h2o.split_frame(ratios=[0.8], seed=42)\n",
    "#     target = y_variable\n",
    "\n",
    "#     st.write(f\"Training rows: {train.nrows}, Testing rows: {test.nrows}\")\n",
    "\n",
    "#     if st.button(\"Run AutoML\"):\n",
    "        \n",
    "#         aml = H2OAutoML(\n",
    "#             max_runtime_secs=1200,\n",
    "#             include_algos=[\"GBM\", \"XGBoost\"],  \n",
    "#             monotone_constraints=monotone_constraints,\n",
    "#             seed=42\n",
    "#         )\n",
    "#         aml.train(x=x_vars, y=y_variable, training_frame=train)\n",
    "        \n",
    "#         top_models = []\n",
    "\n",
    "#         def update_top_models(model_id):\n",
    "#             model = h2o.get_model(model_id)\n",
    "#             preds = model.predict(test).as_data_frame().values.flatten()\n",
    "#             actuals = test[target].as_data_frame().values.flatten()\n",
    "\n",
    "#             mask = ~np.isnan(preds) & ~np.isnan(actuals)\n",
    "#             preds, actuals = preds[mask], actuals[mask]\n",
    "\n",
    "#             if len(preds) == 0:\n",
    "#                 return\n",
    "\n",
    "#             rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "#             mape = mean_absolute_percentage_error(actuals, preds)\n",
    "\n",
    "#             valid_mask = (actuals >= 0) & (preds >= 0)\n",
    "#             rmsle = np.sqrt(mean_squared_error(np.log1p(actuals[valid_mask]), np.log1p(preds[valid_mask]))) if valid_mask.sum() > 0 else np.nan\n",
    "\n",
    "#             model_info = {\n",
    "#                 \"model_id\": model_id,\n",
    "#                 \"rmse\": rmse,\n",
    "#                 \"mse\": mean_squared_error(actuals, preds),\n",
    "#                 \"mae\": np.mean(np.abs(actuals - preds)),\n",
    "#                 \"rmsle\": rmsle,\n",
    "#                 \"mean_residual_deviance\": np.mean((actuals - preds) ** 2),\n",
    "#                 \"mape\": mape\n",
    "#             }\n",
    "\n",
    "#             top_models.append(model_info)\n",
    "#             top_models.sort(key=lambda x: x[\"rmse\"])\n",
    "#             top_models[:] = top_models[:5]\n",
    "\n",
    "#         leaderboard_df = aml.leaderboard.as_data_frame()\n",
    "#         if \"model_id\" in leaderboard_df.columns:\n",
    "#             for model_id in leaderboard_df[\"model_id\"]:\n",
    "#                 update_top_models(model_id)\n",
    "#         else:\n",
    "#             st.warning(\"No models found in leaderboard.\")\n",
    "    \n",
    "#         top_models_df = pd.DataFrame(top_models)\n",
    "#         st.write(\"### AutoML Top 5 Models\")\n",
    "#         st.dataframe(top_models_df)\n",
    "\n",
    "#         if len(top_models) > 0:\n",
    "#             overall_rmse = np.mean([m[\"rmse\"] for m in top_models])\n",
    "#             overall_mape = np.mean([m[\"mape\"] for m in top_models])\n",
    "#             st.write(f\"*Overall RMSE:* {overall_rmse}\")\n",
    "#             st.write(f\"*Overall MAPE:* {overall_mape}\")\n",
    "            \n",
    "#         shap_values = {}\n",
    "#         model_preds = {}\n",
    "\n",
    "#         X_test_df = test.drop(y_variable).as_data_frame()\n",
    "#         y_test = test[y_variable].as_data_frame().values.flatten()\n",
    "\n",
    "#         for model_info in top_models:\n",
    "#             model_id = model_info[\"model_id\"]\n",
    "#             model = h2o.get_model(model_id)\n",
    "\n",
    "#             try:\n",
    "#                 if model.algo not in [\"gbm\", \"xgboost\"]:\n",
    "#                     raise ValueError(f\"Unsupported algorithm: {model.algo}\")\n",
    "\n",
    "#                 contrib = model.predict_contributions(test).as_data_frame()\n",
    "#                 bias = contrib[\"BiasTerm\"]\n",
    "#                 contrib = contrib.drop(\"BiasTerm\", axis=1)\n",
    "#                 contrib = contrib.reindex(columns=X_test_df.columns)\n",
    "#                 shap_values[model_id] = contrib.mean().to_frame(name=model_id).T  \n",
    "\n",
    "#                 model_preds[model_id] = model.predict(test).as_data_frame().values.flatten()\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 st.warning(f\"Failed SHAP for {model_id}: {e}\")\n",
    "\n",
    "#         if shap_values:\n",
    "#             shap_grid_df = pd.concat(shap_values.values(), keys=shap_values.keys()).T\n",
    "#             shap_grid_df.columns.name = None\n",
    "#             st.subheader(\"SHAP Values Grid (mean contributions)\")\n",
    "#             st.dataframe(shap_grid_df.style.format(\"{:.4f}\"), use_container_width=True)\n",
    "\n",
    "#         elasticity_results = {}\n",
    "#         for model_id, shap_df in shap_values.items():\n",
    "#             try:\n",
    "#                 shap_df = shap_df.T.squeeze() if isinstance(shap_df, pd.DataFrame) else shap_df\n",
    "#                 mean_prediction = np.mean(model_preds[model_id])\n",
    "#                 elasticities = shap_df / mean_prediction\n",
    "#                 elasticity_results[model_id] = elasticities\n",
    "#             except Exception as e:\n",
    "#                 st.warning(f\"Elasticity calculation failed for {model_id}: {e}\")\n",
    "\n",
    "#         if elasticity_results:\n",
    "#             elasticity_df = pd.DataFrame(elasticity_results)\n",
    "#             st.subheader(\"Elasticity Grid Across Models\")\n",
    "#             st.dataframe(elasticity_df.style.format(\"{:.5f}\"), use_container_width=True)\n",
    "\n",
    "#             fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#             sns.heatmap(elasticity_df, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n",
    "#             plt.title(\"Elasticities by Feature and Model\")\n",
    "#             st.pyplot(fig)\n",
    "\n",
    "#         st.subheader(\"Actual vs Predicted\")\n",
    "#         for model_id, preds in model_preds.items():\n",
    "#             fig, ax = plt.subplots()\n",
    "#             ax.plot(y_test, label=\"Actual\", color=\"blue\")\n",
    "#             ax.plot(preds, label=\"Predicted\", color=\"red\")\n",
    "#             ax.set_title(f\"Actual vs Predicted for {model_id}\")\n",
    "#             ax.legend()\n",
    "#             st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9d1976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import h2o\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from h2o.automl import H2OAutoML\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.tsa.api import detrend\n",
    "\n",
    "h2o.init(max_mem_size=\"8G\")\n",
    "\n",
    "st.title(\"H2O AutoML with EDA and Feature Engineering\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload CSV Dataset\", type=[\"csv\"])\n",
    "\n",
    "if uploaded_file:\n",
    "    df = pd.read_csv(uploaded_file)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df = df.dropna(subset=[\"Volume\"])  \n",
    "    df[\"Volume\"] = pd.to_numeric(df[\"Volume\"], errors=\"coerce\")\n",
    "    df.dropna(subset=[\"Volume\"], inplace=True)\n",
    "    df = df[df[\"Volume\"] > 0]  \n",
    "\n",
    "    st.write(\"### Filter Data Using L0 and L1\")\n",
    "    col_L0 = st.selectbox(\"Select Feature for L0 Filter\", ['None'] + list(df.columns), key=\"L0_feature\")\n",
    "    if col_L0 != 'None':\n",
    "        unique_vals_L0 = ['None'] + sorted(df[col_L0].dropna().unique())\n",
    "        val_L0 = st.selectbox(f\"Select Value from {col_L0}\", unique_vals_L0, key=\"L0_value\")\n",
    "        if val_L0 != 'None':\n",
    "            df = df[df[col_L0] == val_L0]\n",
    "\n",
    "    col_L1 = st.selectbox(\"Select Feature for L1 Filter\", ['None'] + list(df.columns), key=\"L1_feature\")\n",
    "    if col_L1 != 'None':\n",
    "        unique_vals_L1 = ['None'] + sorted(df[col_L1].dropna().unique())\n",
    "        val_L1 = st.selectbox(f\"Select Value from {col_L1}\", unique_vals_L1, key=\"L1_value\")\n",
    "        if val_L1 != 'None':\n",
    "            df = df[df[col_L1] == val_L1]\n",
    "\n",
    "    st.write(\"Filtered Dataset\", df.head())\n",
    "    st.write(\"Dataset shape before AutoML:\", df.shape)\n",
    "    \n",
    "    drop_cols = [\"D2\", \"D3\", \"D4\", \"D5\", \"D6\", \"AV1\", \"AV2\", \"AV3\", \"AV4\", \"AV5\", \"AV6\", \"EV1\", \"EV2\", \"EV3\", \"EV4\", \"EV5\", \"EV6\"]\n",
    "    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "        df.dropna(subset=[\"Date\"], inplace=True)\n",
    "\n",
    "    if \"SalesValue\" in df.columns and \"Volume\" in df.columns:\n",
    "        if \"ListPrice\" in df.columns:\n",
    "            df[\"Price\"] = df[\"ListPrice\"]\n",
    "        else:\n",
    "            raw_price = df[\"SalesValue\"] / df[\"Volume\"]\n",
    "            df[\"Price\"] = detrend(raw_price.values)\n",
    "            df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            df.dropna(subset=[\"Price\"], inplace=True)\n",
    "        df.drop(columns=[\"SalesValue\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "    df[\"Month\"] = df[\"Date\"].dt.month\n",
    "    df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
    "    df[\"Year\"] = df[\"Date\"].dt.year\n",
    "    df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
    "\n",
    "    st.subheader(\"Target and Feature Selection\")\n",
    "    y_variable = st.selectbox(\"Select Y-variable (Target)\", options=[\"None\"] + list(df.columns), index=0)\n",
    "    if y_variable == \"None\":\n",
    "        st.stop()\n",
    "\n",
    "    x_variables = st.multiselect(\"Select X-variables (Features)\", options=[col for col in df.columns if col != y_variable])\n",
    "    \n",
    "#     st.subheader(\"Target and Feature Selection\")\n",
    "#     y_variable = st.selectbox(\"Select Y-variable (Target)\", options=[\"None\"] + list(df.columns), index=0)\n",
    "#     if y_variable == \"None\":\n",
    "#         st.stop()\n",
    "\n",
    "#     x_variables = st.multiselect(\"Select X-variables (Features)\", options=[col for col in df.columns if col != y_variable])\n",
    "\n",
    "    \n",
    "    st.write(\"### Correlation Heatmap\")\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))  \n",
    "    sns.heatmap(numeric_df.corr(), annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, ax=ax, annot_kws={\"size\": 10})\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    st.subheader(\"Price vs Target Relationship\")\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(data=df, x=\"Price\", y=df.columns[df.columns.get_loc(\"Price\")+1], ax=ax)  # Plot next column after Price\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "\n",
    "\n",
    "    if \"Date\" in df.columns:\n",
    "        st.subheader(\"Time Series Plots\")\n",
    "        with st.expander(\"Optional Time Filters (Year / Month)\", expanded=False):\n",
    "            years = sorted(df[\"Year\"].dropna().unique())\n",
    "            months = sorted(df[\"Month\"].dropna().unique())\n",
    "            selected_year = st.selectbox(\"Select Year\", options=[\"All\"] + list(years), index=0)\n",
    "            selected_month = st.selectbox(\"Select Month\", options=[\"All\"] + list(months), index=0)\n",
    "\n",
    "        ts_df = df.copy()\n",
    "        if selected_year != \"All\":\n",
    "            ts_df = ts_df[ts_df[\"Year\"] == selected_year]\n",
    "        if selected_month != \"All\":\n",
    "            ts_df = ts_df[ts_df[\"Month\"] == selected_month]\n",
    "\n",
    "        grouped_ts = ts_df.groupby(\"Date\")[[y_variable] + x_variables].mean(numeric_only=True).reset_index()\n",
    "\n",
    "        for feature in x_variables:\n",
    "            if feature not in grouped_ts.columns:\n",
    "                st.warning(f\"Skipping feature '{feature}' — not in grouped data.\")\n",
    "                continue\n",
    "            if not pd.api.types.is_numeric_dtype(grouped_ts[feature]):\n",
    "                continue\n",
    "\n",
    "            fig, ax1 = plt.subplots(figsize=(10, 4))\n",
    "            ax1.set_xlabel(\"Date\")\n",
    "            ax1.set_ylabel(y_variable, color=\"tab:red\")\n",
    "            ax1.plot(grouped_ts[\"Date\"], grouped_ts[y_variable], color=\"tab:red\")\n",
    "            ax1.tick_params(axis='y', labelcolor=\"tab:red\")\n",
    "\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.set_ylabel(feature, color=\"tab:blue\")\n",
    "            ax2.plot(grouped_ts[\"Date\"], grouped_ts[feature], color=\"tab:blue\")\n",
    "            ax2.tick_params(axis='y', labelcolor=\"tab:blue\")\n",
    "\n",
    "            fig.tight_layout()\n",
    "            st.pyplot(fig)\n",
    "\n",
    "\n",
    "    st.subheader(\"Outlier Removal\")\n",
    "    outlier_method = st.selectbox(\"Choose method\", [\"None\", \"Z-Score\", \"IQR\"])\n",
    "    initial_size = df.shape[0]\n",
    "\n",
    "    if outlier_method != \"None\":\n",
    "        target_series = df[y_variable].astype(float)\n",
    "        if outlier_method == \"Z-Score\":\n",
    "            z_scores = np.abs(zscore(target_series))\n",
    "            df = df[z_scores < 3]\n",
    "        elif outlier_method == \"IQR\":\n",
    "            Q1, Q3 = target_series.quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            df = df[(target_series >= Q1 - 1.5 * IQR) & (target_series <= Q3 + 1.5 * IQR)]\n",
    "        st.write(f\"Outliers removed: {initial_size - df.shape[0]}\")\n",
    "\n",
    "    st.subheader(\"Feature Transformations\")\n",
    "    feature_transforms = {}\n",
    "    for feature in x_variables:\n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "\n",
    "            if feature == 'Price':\n",
    "                st.warning(f\"Skipping transforms for Price (monotonic constraint)\")\n",
    "                continue\n",
    "            transform = st.selectbox(f\"Transform for {feature}\", [\"None\", \"Log\", \"Power\", \"Standardize\"], key=f\"trans_{feature}\")\n",
    "            feature_transforms[feature] = transform\n",
    "\n",
    "    for feature, method in feature_transforms.items():\n",
    "        try:\n",
    "            if method == \"Log\":\n",
    "                df[feature] = np.log1p(df[feature])\n",
    "            elif method == \"Power\":\n",
    "                df[feature] = np.power(df[feature], 0.5)\n",
    "            elif method == \"Standardize\":\n",
    "                df[feature] = (df[feature] - df[feature].mean()) / df[feature].std()\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not transform {feature}: {e}\")\n",
    "\n",
    "\n",
    "    monotone_constraints = {'Price': -1} if 'Price' in x_variables else None\n",
    "    \n",
    "    df[\"Date\"] = df[\"Date\"].astype(str)\n",
    "    model_cols = list(dict.fromkeys([y_variable] + x_variables + [\"Date\"]))\n",
    "    df_h2o = h2o.H2OFrame(df[model_cols])\n",
    "\n",
    "    for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        if col != \"Date\" and col in df_h2o.columns:\n",
    "            df_h2o[col] = df_h2o[col].asfactor()\n",
    "\n",
    "    x_vars = [col for col in df_h2o.columns if col != y_variable]\n",
    "    train, test = df_h2o.split_frame(ratios=[0.8], seed=42)\n",
    "    target = y_variable\n",
    "\n",
    "    st.write(f\"Training rows: {train.nrows}, Testing rows: {test.nrows}\")\n",
    "\n",
    "    if st.button(\"Run AutoML\"):\n",
    "        aml = H2OAutoML(\n",
    "            max_runtime_secs=1200,\n",
    "            include_algos=[\"GBM\", \"XGBoost\"],\n",
    "            monotone_constraints=monotone_constraints,\n",
    "            seed=42\n",
    "        )\n",
    "        aml.train(x=x_vars, y=y_variable, training_frame=train)\n",
    "        \n",
    "\n",
    "        top_models = []\n",
    "\n",
    "        def update_top_models(model_id):\n",
    "            model = h2o.get_model(model_id)\n",
    "            preds = model.predict(test).as_data_frame().values.flatten()\n",
    "            actuals = test[target].as_data_frame().values.flatten()\n",
    "\n",
    "            mask = ~np.isnan(preds) & ~np.isnan(actuals)\n",
    "            preds, actuals = preds[mask], actuals[mask]\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                return\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "            mape = mean_absolute_percentage_error(actuals, preds)\n",
    "\n",
    "            valid_mask = (actuals >= 0) & (preds >= 0)\n",
    "            rmsle = np.sqrt(mean_squared_error(np.log1p(actuals[valid_mask]), np.log1p(preds[valid_mask]))) if valid_mask.sum() > 0 else np.nan\n",
    "\n",
    "            model_info = {\n",
    "                \"model_id\": model_id,\n",
    "                \"rmse\": rmse,\n",
    "                \"mse\": mean_squared_error(actuals, preds),\n",
    "                \"mae\": np.mean(np.abs(actuals - preds)),\n",
    "                \"rmsle\": rmsle,\n",
    "                \"mean_residual_deviance\": np.mean((actuals - preds) ** 2),\n",
    "                \"mape\": mape\n",
    "            }\n",
    "\n",
    "            top_models.append(model_info)\n",
    "            top_models.sort(key=lambda x: x[\"rmse\"])\n",
    "            top_models[:] = top_models[:5]\n",
    "\n",
    "        leaderboard_df = aml.leaderboard.as_data_frame()\n",
    "        if \"model_id\" in leaderboard_df.columns:\n",
    "            for model_id in leaderboard_df[\"model_id\"]:\n",
    "                update_top_models(model_id)\n",
    "        else:\n",
    "            st.warning(\"No models found in leaderboard.\")\n",
    "    \n",
    "        top_models_df = pd.DataFrame(top_models)\n",
    "        st.write(\"### AutoML Top 5 Models\")\n",
    "        st.dataframe(top_models_df)\n",
    "\n",
    "        if len(top_models) > 0:\n",
    "            overall_rmse = np.mean([m[\"rmse\"] for m in top_models])\n",
    "            overall_mape = np.mean([m[\"mape\"] for m in top_models])\n",
    "            st.write(f\"*Overall RMSE:* {overall_rmse}\")\n",
    "            st.write(f\"*Overall MAPE:* {overall_mape}\")\n",
    "            \n",
    "\n",
    "        shap_values = {}\n",
    "        model_preds = {}\n",
    "\n",
    "        X_test_df = test.drop(y_variable).as_data_frame()\n",
    "        y_test = test[y_variable].as_data_frame().values.flatten()\n",
    "\n",
    "        for model_info in top_models:\n",
    "            model_id = model_info[\"model_id\"]\n",
    "            model = h2o.get_model(model_id)\n",
    "\n",
    "            try:\n",
    "                if model.algo not in [\"gbm\", \"xgboost\"]:\n",
    "                    raise ValueError(f\"Unsupported algorithm: {model.algo}\")\n",
    "\n",
    "                contrib = model.predict_contributions(test).as_data_frame()\n",
    "                bias = contrib[\"BiasTerm\"]\n",
    "                contrib = contrib.drop(\"BiasTerm\", axis=1)\n",
    "                contrib = contrib.reindex(columns=X_test_df.columns)\n",
    "                \n",
    "\n",
    "                if 'Price' in contrib.columns and monotone_constraints and monotone_constraints.get('Price') == -1:\n",
    "                    contrib[\"Price\"] = -np.abs(contrib[\"Price\"])\n",
    "                \n",
    "                shap_values[model_id] = contrib.mean().to_frame(name=model_id).T  \n",
    "                model_preds[model_id] = model.predict(test).as_data_frame().values.flatten()\n",
    "\n",
    "            except Exception as e:\n",
    "                st.warning(f\"Failed SHAP for {model_id}: {e}\")\n",
    "\n",
    "        if shap_values:\n",
    "            shap_grid_df = pd.concat(shap_values.values(), keys=shap_values.keys()).T\n",
    "            shap_grid_df.columns.name = None\n",
    "            st.subheader(\"SHAP Values Grid (mean contributions)\")\n",
    "            st.dataframe(shap_grid_df.style.format(\"{:.4f}\"), use_container_width=True)\n",
    "\n",
    "\n",
    "        elasticity_results = {}\n",
    "        for model_id, shap_df in shap_values.items():\n",
    "            try:\n",
    "                shap_df = shap_df.T.squeeze() if isinstance(shap_df, pd.DataFrame) else shap_df\n",
    "                mean_prediction = np.abs(np.mean(model_preds[model_id])) \n",
    "                elasticities = shap_df / mean_prediction\n",
    "                elasticity_results[model_id] = elasticities\n",
    "            except Exception as e:\n",
    "                st.warning(f\"Elasticity calculation failed for {model_id}: {e}\")\n",
    "\n",
    "        if elasticity_results:\n",
    "            elasticity_df = pd.DataFrame(elasticity_results)\n",
    "            st.subheader(\"Elasticity Grid Across Models\")\n",
    "            st.dataframe(elasticity_df.style.format(\"{:.5f}\"), use_container_width=True)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            sns.heatmap(elasticity_df, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n",
    "            plt.title(\"Elasticities by Feature and Model\")\n",
    "            st.pyplot(fig)\n",
    "\n",
    "\n",
    "        if 'Price' in x_variables:\n",
    "            st.subheader(\"Partial Dependence Plot for Price\")\n",
    "            try:\n",
    "                fig = plt.figure(figsize=(10, 4))\n",
    "                h2o.partial_plot(aml.leader, train, cols=[\"Price\"], plot=True)\n",
    "                st.pyplot(fig)\n",
    "            except Exception as e:\n",
    "                st.warning(f\"Could not generate PDP: {e}\")\n",
    "\n",
    "\n",
    "        st.subheader(\"Actual vs Predicted\")\n",
    "        for model_id, preds in model_preds.items():\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(y_test, label=\"Actual\", color=\"blue\")\n",
    "            ax.plot(preds, label=\"Predicted\", color=\"red\")\n",
    "            ax.set_title(f\"Actual vs Predicted for {model_id}\")\n",
    "            ax.legend()\n",
    "            st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78fe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f2b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678acdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393e038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da5135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
